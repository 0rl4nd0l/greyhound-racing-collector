#!/usr/bin/env python3
"""
ML Data Pipeline Fix Script
===========================

This script addresses the critical data sparsity issue identified in the database audit.
It migrates training data from staging/archive databases to the active database and
retrains the ML models with sufficient data.

Usage:
    python scripts/fix_ml_data_pipeline.py [--dry-run] [--source-db SOURCE]
    
Key Actions:
1. Identify best source database for training data
2. Migrate historical race data to active database  
3. Apply missing database indexes for performance
4. Retrain ML models with sufficient data
5. Validate prediction pipeline functionality
"""

import argparse
import json
import sqlite3
import sys
from datetime import datetime
from pathlib import Path

# Route DB access via helpers
try:
    from scripts.db_utils import open_sqlite_readonly, open_sqlite_writable
except Exception:
    def open_sqlite_readonly(db_path: str | None = None):
        import os as _os, sqlite3 as _sqlite3
        path = db_path or _os.getenv("ANALYTICS_DB_PATH") or _os.getenv("GREYHOUND_DB_PATH") or "greyhound_racing_data.db"
        return _sqlite3.connect(f"file:{Path(path).resolve()}?mode=ro", uri=True)
    def open_sqlite_writable(db_path: str | None = None):
        import os as _os, sqlite3 as _sqlite3
        path = db_path or _os.getenv("STAGING_DB_PATH") or "greyhound_racing_data_stage.db"
        return _sqlite3.connect(str(Path(path).resolve()))


class MLDataPipelineFixer:
    """Fixes the ML training data pipeline by migrating data and retraining models."""

    def __init__(self):
        self.current_db = "greyhound_racing_data.db"
        self.staging_db = "greyhound_racing_data_staging.db"
        self.canonical_db = "databases/canonical_greyhound_data.db"
        self.fixes_log = []

    def log_action(self, action: str, details: dict = None):
        """Log action for audit trail."""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "action": action,
            "details": details or {},
        }
        self.fixes_log.append(log_entry)
        print(f"‚úÖ {action}")
        if details:
            for key, value in details.items():
                print(f"   {key}: {value}")

    def analyze_data_sources(self) -> dict:
        """Analyze available data sources and recommend best option."""
        print("üîç Analyzing available data sources...")

        sources = {}

        # Check current database
        try:
            with sqlite3.connect(self.current_db) as conn:
                cursor = conn.cursor()
                cursor.execute(
                    "SELECT COUNT(*) FROM dog_race_data WHERE finish_position IS NOT NULL"
                )
                training_records = cursor.fetchone()[0]
                sources["current"] = {
                    "path": self.current_db,
                    "training_records": training_records,
                    "usable": training_records > 100,
                }
        except Exception as e:
            sources["current"] = {
                "path": self.current_db,
                "error": str(e),
                "usable": False,
            }

        # Check staging database
        try:
            with open_sqlite_readonly(self.staging_db) as conn:
                cursor = conn.cursor()
                cursor.execute(
                    "SELECT COUNT(*) FROM csv_dog_history_staging WHERE finish_position IS NOT NULL"
                )
                training_records = cursor.fetchone()[0]
                cursor.execute("SELECT COUNT(*) FROM csv_race_metadata_staging")
                race_count = cursor.fetchone()[0]
                sources["staging"] = {
                    "path": self.staging_db,
                    "training_records": training_records,
                    "race_count": race_count,
                    "usable": training_records > 1000,
                }
        except Exception as e:
            sources["staging"] = {
                "path": self.staging_db,
                "error": str(e),
                "usable": False,
            }

        # Check canonical database
        try:
            with open_sqlite_readonly(self.canonical_db) as conn:
                cursor = conn.cursor()
                cursor.execute(
                    "SELECT COUNT(*) FROM dog_performances WHERE finish_position IS NOT NULL"
                )
                training_records = cursor.fetchone()[0]
                cursor.execute("SELECT COUNT(*) FROM race_metadata")
                race_count = cursor.fetchone()[0]
                sources["canonical"] = {
                    "path": self.canonical_db,
                    "training_records": training_records,
                    "race_count": race_count,
                    "usable": training_records > 1000,
                }
        except Exception as e:
            sources["canonical"] = {
                "path": self.canonical_db,
                "error": str(e),
                "usable": False,
            }

        # Determine best source
        usable_sources = {k: v for k, v in sources.items() if v.get("usable", False)}
        if usable_sources:
            best_source = max(
                usable_sources.keys(),
                key=lambda k: sources[k].get("training_records", 0),
            )
            sources["recommended"] = best_source
        else:
            sources["recommended"] = None

        self.log_action("Data source analysis completed", sources)
        return sources

    def apply_missing_indexes(self) -> bool:
        """Apply missing foreign key indexes identified in audit."""
        print("üîß Applying missing database indexes...")

        indexes_to_create = [
            "CREATE INDEX IF NOT EXISTS idx_dog_race_data_race_id ON dog_race_data(race_id);",
            "CREATE INDEX IF NOT EXISTS idx_dog_performances_dog_id ON dog_performances(dog_id);",
            "CREATE INDEX IF NOT EXISTS idx_dogs_ft_extra_dog_id ON dogs_ft_extra(dog_id);",
            "CREATE INDEX IF NOT EXISTS idx_dog_performance_ft_extra_performance_id ON dog_performance_ft_extra(performance_id);",
            "CREATE INDEX IF NOT EXISTS idx_expert_form_analysis_race_id ON expert_form_analysis(race_id);",
        ]

        try:
            with open_sqlite_writable(self.current_db) as conn:
onn:
                cursor = conn.cursor()
                for index_sql in indexes_to_create:
                    cursor.execute(index_sql)
                conn.commit()

            self.log_action(
                "Database indexes applied", {"indexes_count": len(indexes_to_create)}
            )
            return True

        except Exception as e:
            self.log_action("Failed to apply indexes", {"error": str(e)})
            return False

    def migrate_training_data(self, source_db: str, dry_run: bool = False) -> bool:
        """Migrate training data from source database to current database."""
        print(f"üöö Migrating training data from {source_db}...")

        if dry_run:
            print("   [DRY RUN] Would migrate data but not actually executing")
            return True

        try:
            # Strategy depends on source database structure
            if source_db == self.staging_db:
                return self._migrate_from_staging()
            elif source_db == self.canonical_db:
                return self._migrate_from_canonical()
            else:
                self.log_action("Unknown source database", {"source": source_db})
                return False

        except Exception as e:
            self.log_action("Migration failed", {"error": str(e), "source": source_db})
            return False

    def _migrate_from_staging(self) -> bool:
        """Migrate data from staging database format."""
        # This would involve complex ETL from staging tables to production schema
        # For now, we'll recommend using the staging database directly
        self.log_action(
            "Migration strategy",
            {
                "recommendation": "Use staging database directly for training",
                "reason": "Contains most complete dataset (29,762 records)",
                "action": "Update DATABASE_URL to point to staging database",
            },
        )
        return True

    def _migrate_from_canonical(self) -> bool:
        """Migrate data from canonical database format."""
        with sqlite3.connect(self.current_db) as target_conn:
            with sqlite3.connect(self.canonical_db) as source_conn:
                # Copy race metadata
                source_conn.execute("ATTACH DATABASE ? AS target", (self.current_db,))

                # Insert race metadata (if not exists)
                source_conn.execute(
                    """
                    INSERT OR IGNORE INTO target.race_metadata 
                    (race_id, venue, race_number, race_date, distance, grade)
                    SELECT race_id, venue, race_number, race_date, distance, grade 
                    FROM race_metadata
                """
                )

                # Insert dog race data with finish positions
                source_conn.execute(
                    """
                    INSERT OR IGNORE INTO target.dog_race_data
                    (race_id, dog_name, finish_position, box_number, weight, trainer, odds)
                    SELECT p.race_id, p.dog_name, p.finish_position, p.box_number, 
                           p.weight, p.trainer, p.odds
                    FROM dog_performances p
                    WHERE p.finish_position IS NOT NULL
                """
                )

                source_conn.commit()

        self.log_action("Canonical database migration completed")
        return True

    def validate_training_data(self) -> dict:
        """Validate that sufficient training data is now available."""
        print("‚úÖ Validating training data availability...")

        validation = {"sufficient_data": False, "training_records": 0, "issues": []}

        try:
            with sqlite3.connect(self.current_db) as conn:
                cursor = conn.cursor()

                # Check training data count
                cursor.execute(
                    """
                    SELECT COUNT(*) FROM dog_race_data 
                    WHERE finish_position IS NOT NULL 
                    AND finish_position > 0
                """
                )
                training_count = cursor.fetchone()[0]
                validation["training_records"] = training_count

                # Check if sufficient (need at least 1000 for decent training)
                if training_count < 1000:
                    validation["issues"].append(
                        f"Insufficient training data: {training_count} records (need 1000+)"
                    )
                else:
                    validation["sufficient_data"] = True

                # Check data completeness
                cursor.execute(
                    """
                    SELECT 
                        SUM(CASE WHEN dog_name IS NULL THEN 1 ELSE 0 END) as missing_names,
                        SUM(CASE WHEN finish_position IS NULL THEN 1 ELSE 0 END) as missing_positions,
                        SUM(CASE WHEN race_id IS NULL THEN 1 ELSE 0 END) as missing_race_ids
                    FROM dog_race_data
                """
                )
                missing_data = cursor.fetchone()
                if any(count > 0 for count in missing_data):
                    validation["issues"].append(
                        f"Missing critical data: names={missing_data[0]}, positions={missing_data[1]}, race_ids={missing_data[2]}"
                    )

        except Exception as e:
            validation["issues"].append(f"Validation error: {str(e)}")

        self.log_action("Training data validation", validation)
        return validation

    def retrain_models(self, dry_run: bool = False) -> bool:
        """Retrain ML models with available data."""
        print("ü§ñ Retraining ML models...")

        if dry_run:
            print("   [DRY RUN] Would retrain models but not actually executing")
            return True

        try:
            import os
            import subprocess

            # Set up environment
            env = os.environ.copy()
            env["DATABASE_URL"] = f"sqlite:///{self.current_db}"
            env["TESTING"] = "false"

            # Run training script
            result = subprocess.run(
                ["python", "scripts/train_test_model.py"],
                env=env,
                capture_output=True,
                text=True,
                timeout=300,  # 5 minute timeout
            )

            if result.returncode == 0:
                self.log_action(
                    "Model training completed", {"output": result.stdout[-500:]}
                )
                return True
            else:
                self.log_action("Model training failed", {"error": result.stderr})
                return False

        except Exception as e:
            self.log_action("Model training exception", {"error": str(e)})
            return False

    def run_full_fix(self, source_db: str = None, dry_run: bool = False) -> bool:
        """Run the complete ML data pipeline fix."""
        print("üöÄ Starting ML Data Pipeline Fix")
        print("=" * 50)

        success = True

        # Step 1: Analyze data sources
        data_analysis = self.analyze_data_sources()
        if not source_db:
            source_db_key = data_analysis.get("recommended")
            if source_db_key:
                source_db = data_analysis[source_db_key]["path"]
            else:
                print("‚ùå No usable data sources found!")
                return False

        # Step 2: Apply missing indexes
        if not self.apply_missing_indexes():
            print("‚ö†Ô∏è Index creation failed, continuing anyway")

        # Step 3: Migrate training data
        if not self.migrate_training_data(source_db, dry_run):
            print("‚ùå Data migration failed!")
            success = False

        # Step 4: Validate training data
        validation = self.validate_training_data()
        if not validation["sufficient_data"]:
            print("‚ùå Insufficient training data after migration!")
            for issue in validation["issues"]:
                print(f"   ‚Ä¢ {issue}")
            success = False

        # Step 5: Retrain models
        if success and not self.retrain_models(dry_run):
            print("‚ùå Model training failed!")
            success = False

        # Save audit log
        log_file = (
            f"reports/ml_pipeline_fix_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )
        Path(log_file).parent.mkdir(parents=True, exist_ok=True)
        with open(log_file, "w") as f:
            json.dump(
                {
                    "timestamp": datetime.now().isoformat(),
                    "success": success,
                    "source_db": source_db,
                    "dry_run": dry_run,
                    "data_analysis": data_analysis,
                    "validation": validation,
                    "actions": self.fixes_log,
                },
                f,
                indent=2,
            )

        print("\n" + "=" * 50)
        if success:
            print("‚úÖ ML Data Pipeline Fix COMPLETED successfully!")
            print(
                f"üìä Training records available: {validation.get('training_records', 0)}"
            )
            print(f"üìù Audit log saved: {log_file}")
        else:
            print("‚ùå ML Data Pipeline Fix FAILED!")
            print(f"üìù Error log saved: {log_file}")

        return success


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description="Fix ML Data Pipeline")
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Show what would be done without executing",
    )
    parser.add_argument("--source-db", help="Specify source database for migration")

    args = parser.parse_args()

    fixer = MLDataPipelineFixer()
    success = fixer.run_full_fix(source_db=args.source_db, dry_run=args.dry_run)

    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
