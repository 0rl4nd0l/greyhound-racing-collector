#!/usr/bin/env python3
"""
ML System V4 - Temporal Leakage-Safe with Calibration & EV
==========================================================

Features:
- Temporal leakage protection via TemporalFeatureBuilder
- Proper sklearn pipeline with ColumnTransformer
- ExtraTreesClassifier with CalibratedClassifierCV
- Group-normalized probabilities (softmax per race)
- Expected Value (EV) calculation
- Time-ordered train/test splits
- Comprehensive testing and validation
"""

import logging
import sqlite3
import json
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
import warnings
import hashlib
import pickle
import os

# Sklearn imports
from sklearn.model_selection import TimeSeriesSplit
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import accuracy_score, roc_auc_score, brier_score_loss, log_loss
from sklearn.inspection import permutation_importance
import joblib

# Import our temporal feature builder
from temporal_feature_builder import TemporalFeatureBuilder, create_temporal_assertion_hook

# Import profiling infrastructure (disabled due to conflicts)
# from pipeline_profiler import profile_function, track_sequence, pipeline_profiler

# Temporary profiling stub functions to avoid conflicts
def profile_function(func):
    """Disabled profiling decorator - returns function unchanged"""
    return func

def track_sequence(step_name: str, component: str, step_type: str = "processing"):
    """Disabled sequence tracking - returns dummy context manager"""
    class DummyContext:
        def __enter__(self): return self
        def __exit__(self, *args): pass
    return DummyContext()

class DummyProfiler:
    def generate_comprehensive_report(self): pass

pipeline_profiler = DummyProfiler()

logger = logging.getLogger(__name__)

class MLSystemV4:
    """Temporal leakage-safe ML system with calibration and EV calculation."""
    
    def __init__(self, db_path: str = "greyhound_racing_data.db"):
        self.db_path = db_path
        self.temporal_builder = TemporalFeatureBuilder(db_path)
        self.pipeline = None
        self.calibrated_pipeline = None
        self.feature_columns = []
        self.categorical_columns = []
        self.numerical_columns = []
        self.model_info = {}
        self.ev_thresholds = {}

        # Adjust for upcoming races
        self.upcoming_race_box_numbers = True
        self.upcoming_race_weights = True
        self.disable_odds = True
        
        # Create temporal assertion hook
        self.assert_no_leakage = create_temporal_assertion_hook()
        
        # Try to load existing model
        self._try_load_latest_model()
        
        logger.info("üõ°Ô∏è ML System V4 initialized with temporal leakage protection")
    
    def prepare_time_ordered_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Prepare training data with time-ordered splits and comprehensive quality filtering."""
        logger.info("üìÖ Preparing time-ordered training data with quality filtering...")
        
        try:
            conn = sqlite3.connect(self.db_path)
            
            # Load data with temporal information
            query = """
            SELECT 
                d.*,
                r.venue, r.grade, r.distance, r.track_condition, r.weather,
                r.temperature, r.humidity, r.wind_speed, r.field_size,
                r.race_date, r.race_time, r.winner_name, r.winner_odds, r.winner_margin,
                e.pir_rating, e.first_sectional, e.win_time, e.bonus_time
            FROM dog_race_data d
            LEFT JOIN race_metadata r ON d.race_id = r.race_id
            LEFT JOIN enhanced_expert_data e ON d.race_id = e.race_id 
                AND d.dog_clean_name = e.dog_clean_name
            WHERE d.race_id IS NOT NULL 
                AND r.race_date IS NOT NULL
                AND d.finish_position IS NOT NULL
            ORDER BY r.race_date ASC, r.race_time ASC, d.race_id, d.box_number
            """
            
            raw_data = pd.read_sql_query(query, conn)
            conn.close()
            
            if raw_data.empty:
                logger.error("No data available for training")
                return pd.DataFrame(), pd.DataFrame()
            
            logger.info(f"üìä Raw data loaded: {len(raw_data)} samples from {len(raw_data['race_id'].unique())} races")
            
            # Apply comprehensive data quality filtering
            filtered_data = self._apply_data_quality_filters(raw_data)
            
            if filtered_data.empty:
                logger.error("No data remaining after quality filtering")
                return pd.DataFrame(), pd.DataFrame()
            
            # Parse timestamps
            filtered_data['race_timestamp'] = filtered_data.apply(
                lambda row: self.temporal_builder.get_race_timestamp(row), axis=1
            )
            
            # Group by race_id to maintain race integrity
            race_groups = filtered_data.groupby('race_id')
            race_timestamps = race_groups['race_timestamp'].first().sort_values()
            
            # Time-ordered split (80/20)
            split_point = int(len(race_timestamps) * 0.8)
            train_race_ids = set(race_timestamps.iloc[:split_point].index)
            test_race_ids = set(race_timestamps.iloc[split_point:].index)
            
            # Ensure no race_id appears in both train and test
            assert len(train_race_ids.intersection(test_race_ids)) == 0, "Race ID overlap detected!"
            
            train_data = filtered_data[filtered_data['race_id'].isin(train_race_ids)].copy()
            test_data = filtered_data[filtered_data['race_id'].isin(test_race_ids)].copy()
            
            logger.info(f"üìä Time-ordered split:")
            logger.info(f"   Training: {len(train_race_ids)} races, {len(train_data)} samples")
            logger.info(f"   Testing:  {len(test_race_ids)} races, {len(test_data)} samples")
            logger.info(f"   Train period: {train_data['race_timestamp'].min()} to {train_data['race_timestamp'].max()}")
            logger.info(f"   Test period:  {test_data['race_timestamp'].min()} to {test_data['race_timestamp'].max()}")
            
            return train_data, test_data
        
        except Exception as e:
            logger.error(f"Error preparing time-ordered data: {e}")
            return pd.DataFrame(), pd.DataFrame()
    
    def _apply_data_quality_filters(self, raw_data: pd.DataFrame) -> pd.DataFrame:
        """Apply comprehensive data quality filters to ensure clean training data."""
        logger.info("üîç Applying data quality filters...")
        
        initial_races = len(raw_data['race_id'].unique())
        initial_samples = len(raw_data)
        
        # Step 1: Remove races with invalid field sizes (< 3 dogs)
        race_field_sizes = raw_data.groupby('race_id').size()
        valid_field_size_races = race_field_sizes[race_field_sizes >= 3].index
        filtered_data = raw_data[raw_data['race_id'].isin(valid_field_size_races)].copy()
        
        logger.info(f"   Field size filter: {len(filtered_data['race_id'].unique())}/{initial_races} races kept (‚â•3 dogs)")
        
        # Step 2: Remove races with multiple winners
        race_winner_counts = filtered_data[filtered_data['finish_position'] == 1].groupby('race_id').size()
        single_winner_races = race_winner_counts[race_winner_counts == 1].index
        filtered_data = filtered_data[filtered_data['race_id'].isin(single_winner_races)].copy()
        
        logger.info(f"   Winner validation: {len(filtered_data['race_id'].unique())}/{initial_races} races kept (single winner)")
        
        # Step 3: Clean and validate finish positions
        def clean_finish_position(pos):
            """Clean malformed finish position data."""
            if pd.isna(pos):
                return None
            pos_str = str(pos).strip()
            # Remove common suffixes like '=', 'L', 'DSQ', etc.
            pos_cleaned = pos_str.rstrip('=LDSQ')
            try:
                return int(pos_cleaned)
            except (ValueError, TypeError):
                return None
        
        # Clean finish positions
        filtered_data['finish_position_cleaned'] = filtered_data['finish_position'].apply(clean_finish_position)
        
        def validate_finish_positions(group):
            field_size = len(group)
            positions = group['finish_position_cleaned'].dropna()
            
            # Convert to integers, removing any non-convertible values
            valid_positions = []
            for pos in positions:
                try:
                    valid_positions.append(int(pos))
                except (ValueError, TypeError):
                    continue
            
            positions = pd.Series(valid_positions)
            
            # Check if we have enough valid positions
            if len(positions) != field_size:
                return False
            
            # Check if positions are valid (1 to field_size, no duplicates)
            if len(positions) == 0:
                return False
            if positions.min() < 1 or positions.max() > field_size:
                return False
            if len(positions.unique()) != field_size:
                return False
            return True
        
        valid_position_races = []
        for race_id, group in filtered_data.groupby('race_id'):
            if validate_finish_positions(group):
                valid_position_races.append(race_id)
        
        filtered_data = filtered_data[filtered_data['race_id'].isin(valid_position_races)].copy()
        
        # Replace original finish_position with cleaned version
        filtered_data['finish_position'] = filtered_data['finish_position_cleaned']
        filtered_data = filtered_data.drop('finish_position_cleaned', axis=1)
        
        logger.info(f"   Position validation: {len(filtered_data['race_id'].unique())}/{initial_races} races kept (valid positions)")
        
        # Step 4: Remove races with extreme field sizes (>20 dogs - likely data errors)
        race_field_sizes = filtered_data.groupby('race_id').size()
        reasonable_size_races = race_field_sizes[race_field_sizes <= 20].index
        filtered_data = filtered_data[filtered_data['race_id'].isin(reasonable_size_races)].copy()
        
        logger.info(f"   Field size cap: {len(filtered_data['race_id'].unique())}/{initial_races} races kept (‚â§20 dogs)")
        
        # Step 5: Remove races missing critical metadata
        required_fields = ['venue', 'grade', 'distance', 'race_date']
        for field in required_fields:
            before_count = len(filtered_data)
            filtered_data = filtered_data.dropna(subset=[field])
            after_count = len(filtered_data)
            if before_count != after_count:
                logger.info(f"   {field} filter: removed {before_count - after_count} samples with missing {field}")
        
        # Step 6: Ensure balanced class distribution within reasonable bounds
        race_field_sizes = filtered_data.groupby('race_id').size()
        field_size_stats = race_field_sizes.value_counts().sort_index()
        
        logger.info("üìä Final field size distribution:")
        for field_size in sorted(field_size_stats.index):
            count = field_size_stats[field_size]
            percentage = count / len(race_field_sizes) * 100
            expected_win_rate = 1.0 / field_size
            logger.info(f"   {field_size} dogs: {count} races ({percentage:.1f}%) - Expected win rate: {expected_win_rate:.3f}")
        
        # Step 7: Validate win rates by field size
        logger.info("üéØ Validating win rates by field size:")
        for field_size in sorted(field_size_stats.index):
            races_with_size = race_field_sizes[race_field_sizes == field_size].index
            subset = filtered_data[filtered_data['race_id'].isin(races_with_size)]
            actual_win_rate = (subset['finish_position'] == 1).mean()
            expected_win_rate = 1.0 / field_size
            difference = abs(actual_win_rate - expected_win_rate)
            
            status = "‚úÖ" if difference < 0.02 else "‚ö†Ô∏è" if difference < 0.05 else "‚ùå"
            logger.info(f"   {field_size} dogs: {actual_win_rate:.4f} actual vs {expected_win_rate:.4f} expected {status}")
        
        final_races = len(filtered_data['race_id'].unique())
        final_samples = len(filtered_data)
        
        logger.info(f"‚úÖ Data quality filtering complete:")
        logger.info(f"   Races: {initial_races} ‚Üí {final_races} ({final_races/initial_races*100:.1f}% kept)")
        logger.info(f"   Samples: {initial_samples} ‚Üí {final_samples} ({final_samples/initial_samples*100:.1f}% kept)")
        
        return filtered_data
    
    def build_leakage_safe_features(self, raw_data: pd.DataFrame) -> pd.DataFrame:
        """Build features using temporal feature builder to prevent leakage."""
        try:
            # with track_sequence('feature_engineering', 'MLSystemV4', 'feature_engineering'):
                logger.info("üîß Building leakage-safe features...")
                
                if raw_data.empty:
                    logger.error("Empty input data for feature building")
                    return pd.DataFrame()
                
                all_features = []
                failed_races = []
                
                # Process each race separately
                for race_id in raw_data['race_id'].unique():
                    try:
                        race_data = raw_data[raw_data['race_id'] == race_id].copy()
                        
                        # Build temporal leakage-safe features
                        if self.upcoming_race_box_numbers:
                            race_data.loc[:, 'box_number'] = range(1, len(race_data) + 1)

                        if self.upcoming_race_weights and 'weight' in race_data.columns:
                            # Weight column already exists, no need to map
                            pass

                        race_features = self.temporal_builder.build_features_for_race(race_data, race_id)
                        
                        if race_features is None or race_features.empty:
                            logger.warning(f"No features generated for race {race_id}")
                            failed_races.append(race_id)
                            continue

                        # Validate temporal integrity
                        self.temporal_builder.validate_temporal_integrity(race_features, race_data)
                        
                        all_features.append(race_features)
                        
                    except Exception as e:
                        logger.error(f"Error building features for race {race_id}: {e}")
                        failed_races.append(race_id)
                        continue
                
                if not all_features:
                    logger.error("No features could be built for any races")
                    return pd.DataFrame()
                
                # Combine all race features
                combined_features = pd.concat(all_features, ignore_index=True)
                
                if failed_races:
                    logger.warning(f"Failed to build features for {len(failed_races)} races: {failed_races[:5]}...")
                
                logger.info(f"‚úÖ Built {len(combined_features)} feature vectors across {len(all_features)} races (failed: {len(failed_races)})")
                
                return combined_features
                
        except Exception as e:
            logger.error(f"Critical error in feature building: {e}")
            import traceback
            traceback.print_exc()
            return pd.DataFrame()
    
    def create_sklearn_pipeline(self, features_df: pd.DataFrame) -> Pipeline:
        """Create sklearn pipeline with proper encoding and calibration."""
        logger.info("üèóÔ∏è Creating sklearn pipeline with calibration...")
        
        # Identify feature types
        self.categorical_columns = [
            col for col in features_df.columns 
            if col in ['venue', 'grade', 'track_condition', 'weather', 'trainer_name']
        ]
        
        # Exclude metadata columns and non-numeric columns from training
        exclude_columns = ['race_id', 'dog_clean_name', 'target', 'target_timestamp', 'race_date', 'race_time']
        
        self.numerical_columns = [
            col for col in features_df.columns 
            if col not in self.categorical_columns + exclude_columns and 
               pd.api.types.is_numeric_dtype(features_df[col])
        ]
        
        logger.info(f"üìä Feature composition:")
        logger.info(f"   Categorical: {len(self.categorical_columns)} features")
        logger.info(f"   Numerical: {len(self.numerical_columns)} features")
        
        # Create column transformer
        preprocessor = ColumnTransformer(
            transformers=[
                ('num', 'passthrough', self.numerical_columns),
                ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), self.categorical_columns)
            ],
            remainder='drop'
        )
        
        # Create base model with enhanced hyperparameters for class imbalance
        base_model = ExtraTreesClassifier(
            n_estimators=1000,  # More trees for better performance
            min_samples_leaf=2,  # Lower for better learning on minority class
            max_depth=20,  # Deeper trees for complex patterns
            max_features='sqrt',  # Standard feature sampling
            class_weight='balanced',  # Handle class imbalance
            bootstrap=True,  # Enable bootstrap for better generalization
            random_state=42,
            n_jobs=-1
        )
        
        # Create pipeline
        pipeline = Pipeline([
            ('preprocessor', preprocessor),
            ('classifier', base_model)
        ])
        
        # Wrap with calibration using stratified splits for better handling of imbalanced data
        calibrated_pipeline = CalibratedClassifierCV(
            pipeline, 
            method='isotonic',  # Preferred over sigmoid for non-parametric calibration
            cv=5  # More folds for better calibration with sufficient data
        )
        
        return calibrated_pipeline
    
    def train_model(self) -> bool:
        """Train the complete leakage-safe, calibrated model."""
        logger.info("üöÄ Starting leakage-safe model training...")
        
        # Prepare time-ordered data
        train_data, test_data = self.prepare_time_ordered_data()
        if train_data.empty or test_data.empty:
            logger.error("No data available for training")
            return False
        
        # Build leakage-safe features
        logger.info("Building features for training data...")
        train_features = self.build_leakage_safe_features(train_data)
        
        logger.info("Building features for test data...")
        test_features = self.build_leakage_safe_features(test_data)
        
        if train_features is None or test_features is None:
            logger.error("Feature building returned None")
            return False
            
        if train_features.empty or test_features.empty:
            logger.error("No features created")
            return False
        
        # Prepare training data
        X_train = train_features.drop(['race_id', 'dog_clean_name', 'target', 'target_timestamp'], axis=1, errors='ignore')
        y_train = train_features['target']
        X_test = test_features.drop(['race_id', 'dog_clean_name', 'target', 'target_timestamp'], axis=1, errors='ignore')
        y_test = test_features['target']
        
        # Store feature columns for later use
        self.feature_columns = X_train.columns.tolist()
        
        logger.info(f"üìä Training data shape: {X_train.shape}")
        logger.info(f"üìä Test data shape: {X_test.shape}")
        logger.info(f"üéØ Target distribution - Train wins: {y_train.sum()}/{len(y_train)} ({y_train.mean():.3f})")
        logger.info(f"üéØ Target distribution - Test wins: {y_test.sum()}/{len(y_test)} ({y_test.mean():.3f})")
        
        # Create and train pipeline
        self.calibrated_pipeline = self.create_sklearn_pipeline(X_train)
        
        # Train the model
        logger.info("üéØ Training calibrated model...")
        self.calibrated_pipeline.fit(X_train, y_train)
        
        # Evaluate model
        train_pred_proba = self.calibrated_pipeline.predict_proba(X_train)[:, 1]
        test_pred_proba = self.calibrated_pipeline.predict_proba(X_test)[:, 1]
        
        train_pred = (train_pred_proba > 0.5).astype(int)
        test_pred = (test_pred_proba > 0.5).astype(int)
        
        # Calculate metrics
        train_accuracy = accuracy_score(y_train, train_pred)
        test_accuracy = accuracy_score(y_test, test_pred)
        train_auc = roc_auc_score(y_train, train_pred_proba)
        test_auc = roc_auc_score(y_test, test_pred_proba)
        train_brier = brier_score_loss(y_train, train_pred_proba)
        test_brier = brier_score_loss(y_test, test_pred_proba)
        
        logger.info("üìà Model Performance:")
        logger.info(f"   Training Accuracy: {train_accuracy:.4f}")
        logger.info(f"   Test Accuracy: {test_accuracy:.4f}")
        logger.info(f"   Training AUC: {train_auc:.4f}")
        logger.info(f"   Test AUC: {test_auc:.4f}")
        logger.info(f"   Training Brier Score: {train_brier:.4f}")
        logger.info(f"   Test Brier Score: {test_brier:.4f}")
        
        # Feature importance
        try:
            # Get feature names after preprocessing
            feature_names = self._get_feature_names_after_preprocessing()
            
            # Calculate permutation importance
            perm_importance = permutation_importance(
                self.calibrated_pipeline, X_test, y_test, 
                n_repeats=5, random_state=42, n_jobs=-1
            )
            
            # Log top features
            importance_df = pd.DataFrame({
                'feature': feature_names,
                'importance': perm_importance.importances_mean
            }).sort_values('importance', ascending=False)
            
            logger.info("üîç Top 10 Feature Importances:")
            for _, row in importance_df.head(10).iterrows():
                logger.info(f"   {row['feature']}: {row['importance']:.4f}")
        
        except Exception as e:
            logger.warning(f"Could not calculate feature importance: {e}")
        
        # Learn EV thresholds
        self.ev_thresholds = self._learn_ev_thresholds(test_features, test_pred_proba)
        
        # Save model info
        self.model_info = {
            'model_type': 'ExtraTreesClassifier_Calibrated',
            'train_accuracy': train_accuracy,
            'test_accuracy': test_accuracy,
            'train_auc': train_auc,
            'test_auc': test_auc,
            'train_brier': train_brier,
            'test_brier': test_brier,
            'n_features': len(self.feature_columns),
            'n_train_samples': len(X_train),
            'n_test_samples': len(X_test),
            'temporal_split': True,
            'calibration_method': 'isotonic',
            'ev_thresholds': self.ev_thresholds,
            'trained_at': datetime.now().isoformat()
        }
        
        # Save model
        model_path = self._save_model()
        logger.info(f"‚úÖ Model training completed! Saved to {model_path}")
        
        return True
    
    def preprocess_upcoming_race_csv(self, race_data: pd.DataFrame, race_id: str) -> pd.DataFrame:
        """Preprocess upcoming race CSV to match expected format."""
        try:
            # Map CSV columns to expected database columns
            column_mapping = {
                'Dog Name': 'dog_clean_name',
                'BOX': 'box_number', 
                'WGT': 'weight',
                'DIST': 'distance',
                'DATE': 'race_date',
                'TRACK': 'venue',
                'G': 'grade',
                'PIR': 'pir_rating',
                'SP': 'starting_price'
            }
            
            # Rename columns
            processed_data = race_data.rename(columns=column_mapping)
            
            # Extract race information from race_id 
            # Format: "Race 1 - AP_K - 2025-08-04"
            race_parts = race_id.split(' - ')
            if len(race_parts) >= 3:
                race_date = race_parts[2]
                venue_code = race_parts[1]
                
                # Add race metadata
                processed_data['race_date'] = race_date
                processed_data['venue'] = venue_code
                processed_data['race_id'] = race_id
                
                # Add race_time if not present (use noon as default for upcoming races)
                processed_data['race_time'] = '12:00'
            
            # Clean dog names (remove numbering like "1. ", "2. ")
            if 'dog_clean_name' in processed_data.columns:
                processed_data['dog_clean_name'] = processed_data['dog_clean_name'].str.replace(r'^\d+\.\s*', '', regex=True)
                processed_data['dog_clean_name'] = processed_data['dog_clean_name'].str.strip()
            
            # Clean box numbers
            if 'box_number' in processed_data.columns:
                processed_data['box_number'] = pd.to_numeric(processed_data['box_number'], errors='coerce')
            
            # Clean weight
            if 'weight' in processed_data.columns:
                processed_data['weight'] = pd.to_numeric(processed_data['weight'], errors='coerce')
            
            # Clean distance
            if 'distance' in processed_data.columns:
                processed_data['distance'] = pd.to_numeric(processed_data['distance'], errors='coerce')
            
            # Add required fields that might be missing
            required_fields = {
                'finish_position': None,  # Upcoming race - no finish position yet
                'individual_time': None,  # Upcoming race - no time yet
                'field_size': len(processed_data),
                'track_condition': 'Good',  # Default track condition
                'weather': 'Fine',  # Default weather
                'temperature': 20.0,  # Default temperature
                'humidity': 50.0,  # Default humidity
                'wind_speed': 0.0  # Default wind speed
            }
            
            for field, default_value in required_fields.items():
                if field not in processed_data.columns:
                    processed_data[field] = default_value
            
            logger.info(f"üìã Preprocessed {len(processed_data)} dogs for race {race_id}")
            return processed_data
            
        except Exception as e:
            logger.error(f"Error preprocessing CSV for race {race_id}: {e}")
            raise
    
    def predict_race(self, race_data: pd.DataFrame, race_id: str, 
                    market_odds: Dict[str, float] = None) -> Dict[str, Any]:
        """Make predictions for all dogs in a race with group normalization and EV."""
        if not self.calibrated_pipeline:
            logger.error("No calibrated model available for prediction")
            return {'success': False, 'error': 'No model loaded'}
        
        try:
            # Build leakage-safe features for the race with robust error handling
            try:
                race_features = self.temporal_builder.build_features_for_race(race_data, race_id)
                
                if race_features is None or race_features.empty:
                    logger.error(f"No features could be built for race {race_id}")
                    return {
                        'success': False, 
                        'error': 'Feature building returned empty result',
                        'race_id': race_id,
                        'fallback_reason': 'Feature building failed - no features generated'
                    }
                
                # Validate temporal integrity (critical assertion)
                self.temporal_builder.validate_temporal_integrity(race_features, race_data)
                
            except Exception as feature_error:
                logger.error(f"Feature building failed for race {race_id}: {feature_error}")
                return {
                    'success': False,
                    'error': f'Feature building error: {str(feature_error)}',
                    'race_id': race_id,
                    'fallback_reason': f'Feature building pipeline error: {str(feature_error)}'
                }
            
            # Prepare features for prediction
            X_pred = race_features.drop(['race_id', 'dog_clean_name', 'target', 'target_timestamp'], 
                                       axis=1, errors='ignore')
            
            # Debug: Show what features we have vs what the model expects
            logger.debug(f"Features from temporal builder: {X_pred.columns.tolist()}")
            logger.debug(f"Model expects features: {self.feature_columns}")
            
            # Check for feature column mismatch
            missing_features = set(self.feature_columns) - set(X_pred.columns)
            extra_features = set(X_pred.columns) - set(self.feature_columns)
            
            if missing_features:
                logger.warning(f"Missing features (will be filled with 0): {missing_features}")
            if extra_features:
                logger.warning(f"Extra features (will be dropped): {extra_features}")
            
            # Ensure all required columns are present with proper data types
            X_pred = X_pred.reindex(columns=self.feature_columns, fill_value=0)
            
            # Handle features based on their expected type in the trained model
            categorical_features_in_model = ['venue', 'grade', 'track_condition', 'weather', 'trainer_name']
            numerical_features = [col for col in X_pred.columns if col not in categorical_features_in_model]
            
            logger.debug(f"Processing {len(categorical_features_in_model)} categorical and {len(numerical_features)} numerical features")
            
            # Step 1: Handle categorical features
            for cat_col in categorical_features_in_model:
                if cat_col in X_pred.columns:
                    # Replace 0 values with appropriate defaults for categorical features
                    default_cat_values = {
                        'venue': 'UNKNOWN',
                        'grade': '5',
                        'track_condition': 'Good',
                        'weather': 'Fine', 
                        'trainer_name': 'Unknown'
                    }
                    
                    # Convert to string and replace zeros with defaults
                    X_pred[cat_col] = X_pred[cat_col].apply(
                        lambda x: default_cat_values.get(cat_col, 'Unknown') if (pd.isna(x) or x == 0 or x == '0') else str(x)
                    )
                    logger.debug(f"Set categorical defaults for {cat_col}: {X_pred[cat_col].unique()}")
            
            # Step 2: Handle numerical features with robust error handling
            logger.debug(f"Converting {len(numerical_features)} numerical features")
            for col in numerical_features:
                if col in X_pred.columns:
                    try:
                        original_dtype = X_pred[col].dtype
                        original_values = X_pred[col].copy()
                        
                        # Convert to numeric, coercing errors to NaN
                        X_pred[col] = pd.to_numeric(X_pred[col], errors='coerce')
                        
                        # Replace NaN with 0, ensuring numeric type
                        X_pred[col] = X_pred[col].fillna(0.0).astype(np.float64)
                        
                        if original_dtype == 'object':
                            logger.debug(f"Converted numerical column {col} from {original_dtype} to float64")
                            # Log non-convertible values for debugging
                            invalid_mask = pd.isna(pd.to_numeric(original_values, errors='coerce'))
                            if invalid_mask.any() and len(original_values[invalid_mask].unique()) > 0:
                                invalid_values = original_values[invalid_mask].unique()[:3]
                                logger.debug(f"  Non-numeric values in {col}: {invalid_values.tolist()}")
                                
                    except Exception as e:
                        logger.error(f"Error converting numerical column {col}: {e}")
                        # Force to float64 with zeros as fallback
                        X_pred[col] = np.zeros(len(X_pred), dtype=np.float64)
            
            # Final validation
            logger.debug(f"Feature preparation complete. Shape: {X_pred.shape}")
            logger.debug(f"Categorical columns: {[col for col in categorical_features_in_model if col in X_pred.columns]}")
            logger.debug(f"Numerical columns: {[col for col in numerical_features if col in X_pred.columns]}")
            
            # Assert no temporal leakage at prediction time
            for idx, row in race_features.iterrows():
                dog_name = row['dog_clean_name']
                # Use original race features for temporal assertion (before numerical conversion)
                original_features_dict = row.to_dict()
                self.assert_no_leakage(original_features_dict, race_id, dog_name)
            
            # Make raw predictions with instrumentation (inference)
            # with track_sequence('inference', 'MLSystemV4', 'inference'):
            calibration_present = 'calibration_meta' in self.model_info
            predict_proba_lambda = lambda x: self.calibrated_pipeline.predict_proba(x)[:, 1]
            raw_win_probabilities = predict_proba_lambda(X_pred)
            
            # Record calibration stage
            if calibration_present:
                # with track_sequence('calibration', 'MLSystemV4', 'calibration'):
                logger.info('Recording calibration stage after predict_proba as calibration is present.')
            
            # Group normalization (softmax within race)
            normalized_win_probs = self._group_normalize_probabilities(raw_win_probabilities)
            
            # Validate normalization
            prob_sum = np.sum(normalized_win_probs)
            if not (0.95 <= prob_sum <= 1.05):
                logger.warning(f"Normalization check failed: sum = {prob_sum:.4f}")
            
            # Calculate place probabilities (heuristic)
            normalized_place_probs = np.minimum(0.95, normalized_win_probs * 2.8)
            
            # Calculate EV if market odds available
            ev_calculations = {}
            if market_odds:
                for i, dog_name in enumerate(race_features['dog_clean_name']):
                    if dog_name in market_odds:
                        odds = market_odds[dog_name]
                        win_prob = normalized_win_probs[i]
                        ev_win = win_prob * (odds - 1) - (1 - win_prob)
                        ev_calculations[dog_name] = {
                            'odds': odds,
                            'ev_win': ev_win,
                            'ev_positive': ev_win > 0
                        }
            
            # Create prediction results
            predictions = []
            for i, row in race_features.iterrows():
                dog_name = row['dog_clean_name']
                
                confidence_value = self._calculate_prediction_confidence(X_pred.iloc[i])
                
                prediction = {
                    'dog_name': dog_name,
                    'dog_clean_name': dog_name,  # Add this mapping for backward compatibility
                    'box_number': row.get('box_number', i + 1),
                    'win_prob_raw': float(raw_win_probabilities[i]),
                    'win_prob_norm': float(normalized_win_probs[i]),
                    'place_prob_norm': float(normalized_place_probs[i]),
                    'confidence': confidence_value,
                    'confidence_level': self._get_confidence_description(confidence_value),
                    'calibration_applied': True
                }
                
                # Add EV if available
                if dog_name in ev_calculations:
                    prediction.update(ev_calculations[dog_name])
                
                predictions.append(prediction)
            
            # Sort by normalized win probability
            predictions.sort(key=lambda x: x['win_prob_norm'], reverse=True)
            
            # Add ranking
            for i, pred in enumerate(predictions):
                pred['predicted_rank'] = i + 1
            
            # Calculate explainability metadata
            explainability_meta = self._create_explainability_metadata(X_pred, race_id)
            
            result = {
                'success': True,
                'race_id': race_id,
                'predictions': predictions,
                'model_info': self.model_info.get('model_type', 'unknown'),
                'calibration_meta': {
                    'method': 'isotonic',
                    'applied': True,
                    'normalization_sum': float(prob_sum)
                },
                'explainability_meta': explainability_meta,
                'ev_meta': {
                    'thresholds': self.ev_thresholds,
                    'calculations_available': len(ev_calculations) > 0
                },
                'timestamp': datetime.now().isoformat()
            }
            
            logger.info(f"‚úÖ Race prediction complete for {race_id}: {len(predictions)} dogs")
            
            # Expose internal profiler flush
            pipeline_profiler.generate_comprehensive_report()
            
            return result
        
        except Exception as e:
            logger.error(f"Error predicting race {race_id}: {e}")
            import traceback
            logger.error(f"Full traceback: {traceback.format_exc()}")
            return {
                'success': False,
                'error': str(e),
                'race_id': race_id,
                'fallback_reason': f'Prediction pipeline error: {str(e)}'
            }
    
    def _group_normalize_probabilities(self, probabilities: np.ndarray) -> np.ndarray:
        """Apply enhanced normalization to preserve variance while ensuring probabilities sum to 1."""
        # with track_sequence('post', 'MLSystemV4', 'post'):
        # Check the variance in raw probabilities
        prob_variance = np.var(probabilities)
        prob_range = np.max(probabilities) - np.min(probabilities)
        
        # Choose normalization method based on variance
        if prob_range < 0.03:  # Very low variance - use power transformation to amplify differences
            logger.debug(f"Low variance detected ({prob_range:.6f}), using power transformation")
            # Apply power transformation to amplify differences
            power = 2.5  # Higher power to amplify differences more
            powered_probs = np.power(probabilities, power)
            normalized = powered_probs / np.sum(powered_probs)
        elif prob_range < 0.10:  # Moderate variance - use enhanced temperature softmax
            logger.debug(f"Moderate variance detected ({prob_range:.6f}), using temperature softmax")
            # Use higher temperature to preserve more variance  
            temperature = 3.0  # Higher temperature preserves more variance
            temp_probs = np.exp((probabilities - np.max(probabilities)) / temperature)
            normalized = temp_probs / np.sum(temp_probs)
        else:  # High variance - simple normalization to preserve it
            logger.debug(f"High variance detected ({prob_range:.6f}), using simple normalization")
            # Simple normalization preserves the most variance
            normalized = probabilities / np.sum(probabilities)
        
        # Log normalization results
        final_range = np.max(normalized) - np.min(normalized)
        logger.debug(f"Normalization: {prob_range:.6f} -> {final_range:.6f} (variance preserved: {final_range/prob_range*100:.1f}%)")
        
        return normalized
    
    def _learn_ev_thresholds(self, test_features: pd.DataFrame, 
                           test_probabilities: np.ndarray) -> Dict[str, float]:
        """Learn optimal EV thresholds based on ROI optimization."""
        logger.info("üìä Learning EV thresholds...")
        
        # Simulate market odds (would be replaced with real odds in production)
        simulated_odds = 1.0 / (test_probabilities + 0.01)  # Inverse probability with small epsilon
        
        # Test different EV thresholds
        thresholds = np.arange(0.0, 0.15, 0.01)
        best_roi = -float('inf')
        best_threshold = 0.05
        
        for threshold in thresholds:
            # Calculate EV for each prediction
            ev_values = test_probabilities * (simulated_odds - 1) - (1 - test_probabilities)
            
            # Select bets above threshold
            bet_mask = ev_values > threshold
            
            if np.sum(bet_mask) == 0:
                continue
            
            # Calculate ROI (simplified)
            actual_outcomes = test_features['target'].values
            bet_outcomes = actual_outcomes[bet_mask]
            bet_odds = simulated_odds[bet_mask]
            
            # Calculate returns
            returns = np.where(bet_outcomes == 1, bet_odds - 1, -1)
            roi = np.mean(returns) if len(returns) > 0 else -1
            
            if roi > best_roi:
                best_roi = roi
                best_threshold = threshold
        
        thresholds_dict = {
            'win_threshold': float(best_threshold),
            'place_threshold': float(best_threshold * 0.7),  # Lower threshold for place bets
            'quinella_threshold': float(best_threshold * 1.2),  # Higher threshold for exotic bets
            'learned_roi': float(best_roi)
        }
        
        logger.info(f"üìà Learned EV thresholds: {thresholds_dict}")
        return thresholds_dict
    
    def _calculate_prediction_confidence(self, features: pd.Series) -> float:
        """Calculate confidence based on feature completeness and model certainty."""
        # Feature completeness
        non_zero_features = np.sum(features != 0)
        total_features = len(features)
        completeness = non_zero_features / total_features
        
        # Return scaled confidence
        return min(0.95, completeness * 0.8 + 0.2)
    
    def _get_confidence_description(self, confidence_value: float) -> str:
        """Convert confidence score to descriptive level."""
        if confidence_value >= 0.8:
            return "High"
        elif confidence_value >= 0.6:
            return "Medium"
        elif confidence_value >= 0.4:
            return "Low"
        else:
            return "Very Low"
    
    def _create_explainability_metadata(self, X_pred: pd.DataFrame, race_id: str) -> Dict[str, Any]:
        """Create explainability metadata for logging."""
        try:
            # Calculate mean feature values for numeric columns only
            numeric_cols = X_pred.select_dtypes(include=[np.number]).columns
            mean_values = {}
            
            if len(numeric_cols) > 0:
                mean_values = X_pred[numeric_cols].mean().to_dict()
            
            # Add info about non-numeric columns
            non_numeric_cols = X_pred.select_dtypes(exclude=[np.number]).columns
            non_numeric_info = {}
            for col in non_numeric_cols:
                unique_vals = X_pred[col].unique()
                non_numeric_info[col] = {
                    'type': 'categorical',
                    'unique_count': len(unique_vals),
                    'sample_values': unique_vals[:3].tolist() if len(unique_vals) > 0 else []
                }
            
            feature_summary = {
                'mean_values': mean_values,
                'non_numeric_features': non_numeric_info,
                'feature_count': len(X_pred.columns),
                'numeric_feature_count': len(numeric_cols),
                'samples_count': len(X_pred),
                'log_path': f'logs/explainability_{race_id}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
            }
            
            # Save detailed explainability to logs
            logs_dir = Path('logs')
            logs_dir.mkdir(exist_ok=True)
            
            with open(logs_dir / f'explainability_{race_id}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json', 'w') as f:
                json.dump({
                    'race_id': race_id,
                    'timestamp': datetime.now().isoformat(),
                    'feature_values': X_pred.to_dict('records'),
                    'feature_names': X_pred.columns.tolist()
                }, f, indent=2)
            
            return feature_summary
        
        except Exception as e:
            logger.warning(f"Could not create explainability metadata: {e}")
            return {'error': str(e)}
    
    def _get_feature_names_after_preprocessing(self) -> List[str]:
        """Get feature names after preprocessing pipeline."""
        try:
            # Get the fitted preprocessor
            preprocessor = self.calibrated_pipeline.base_estimator_.named_steps['preprocessor']
            
            # Numerical features (passthrough)
            num_features = self.numerical_columns
            
            # Categorical features (after one-hot encoding)
            cat_encoder = preprocessor.named_transformers_['cat']
            cat_features = []
            for i, col in enumerate(self.categorical_columns):
                if hasattr(cat_encoder, 'categories_'):
                    categories = cat_encoder.categories_[i]
                    cat_features.extend([f"{col}_{cat}" for cat in categories])
            
            return num_features + cat_features
        
        except Exception as e:
            logger.warning(f"Could not get feature names: {e}")
            return self.feature_columns
    
    def _save_model(self) -> Path:
        """Save the complete model pipeline."""
        model_dir = Path('./ml_models_v4')
        model_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        model_path = model_dir / f'ml_model_v4_{timestamp}.joblib'
        
        model_data = {
            'calibrated_pipeline': self.calibrated_pipeline,
            'feature_columns': self.feature_columns,
            'categorical_columns': self.categorical_columns,
            'numerical_columns': self.numerical_columns,
            'model_info': self.model_info,
            'ev_thresholds': self.ev_thresholds,
            'saved_at': datetime.now().isoformat()
        }
        
        joblib.dump(model_data, model_path)
        logger.info(f"üíæ Model saved to {model_path}")
        
        return model_path
    
    def _create_lightweight_mock_model(self):
        """Create lightweight mock model when no calibrated_pipeline is detected on disk."""
        logger.info("üîß No calibrated_pipeline detected on disk, creating lightweight mock model...")
        
        try:
            # Call existing logic from test_prediction_only
            from test_prediction_only import create_mock_trained_model
            success = create_mock_trained_model(self)
            
            if success:
                logger.info("‚úÖ Lightweight mock model created successfully")
                logger.info("   This avoids re-training while still exercising preprocessing, ColumnTransformer, calibration layers, and EV logic")
            else:
                logger.error("‚ùå Failed to create lightweight mock model")
                
        except ImportError as e:
            logger.error(f"Could not import create_mock_trained_model: {e}")
            logger.info("Falling back to basic mock model creation...")
            self._create_basic_mock_model()
        except Exception as e:
            logger.error(f"Error creating lightweight mock model: {e}")
            logger.info("Falling back to basic mock model creation...")
            self._create_basic_mock_model()
    
    def _create_basic_mock_model(self):
        """Create a basic mock model as fallback."""
        logger.info("üì¶ Creating basic mock model as fallback...")
        
        # Create a simple mock feature set
        self.feature_columns = [
            'box_number', 'weight', 'distance', 'historical_avg_position',
            'historical_win_rate', 'venue_specific_avg_position', 'days_since_last_race'
        ]
        
        self.numerical_columns = [
            'box_number', 'weight', 'distance', 'historical_avg_position',
            'historical_win_rate', 'venue_specific_avg_position', 'days_since_last_race'
        ]
        
        self.categorical_columns = ['venue']
        
        # Create a minimal pipeline
        preprocessor = ColumnTransformer(
            transformers=[
                ('num', 'passthrough', self.numerical_columns),
                ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), self.categorical_columns)
            ],
            remainder='drop'
        )
        
        base_model = ExtraTreesClassifier(n_estimators=10, random_state=42)
        pipeline = Pipeline([
            ('preprocessor', preprocessor),
            ('classifier', base_model)
        ])
        
        # Create mock training data
        n_samples = 50
        mock_X = pd.DataFrame({
            'box_number': np.random.randint(1, 9, n_samples),
            'weight': np.random.uniform(28, 35, n_samples),
            'distance': np.random.choice([400, 500, 600], n_samples),
            'historical_avg_position': np.random.uniform(1, 8, n_samples),
            'historical_win_rate': np.random.uniform(0, 0.3, n_samples),
            'venue_specific_avg_position': np.random.uniform(1, 8, n_samples),
            'days_since_last_race': np.random.uniform(7, 30, n_samples),
            'venue': np.random.choice(['DAPT', 'GEE', 'WAR'], n_samples)
        })
        
        mock_y = np.random.choice([0, 1], n_samples, p=[0.875, 0.125])  # Realistic win rate
        
        # Train the mock model
        calibrated_pipeline = CalibratedClassifierCV(pipeline, method='isotonic', cv=3)
        calibrated_pipeline.fit(mock_X, mock_y)
        
        self.calibrated_pipeline = calibrated_pipeline
        self.model_info = {
            'model_type': 'Mock_ExtraTreesClassifier_Calibrated',
            'test_accuracy': 0.85,
            'test_auc': 0.70,
            'trained_at': datetime.now().isoformat()
        }
        
        logger.info("‚úÖ Basic mock model created successfully")
    
    def _try_load_latest_model(self):
        """Try to load the latest model."""
        # with track_sequence('model_load', 'MLSystemV4', 'model_load'):
        model_dir = Path('./ml_models_v4')
        if not model_dir.exists():
            logger.info("No model directory found")
            self._create_lightweight_mock_model()
            return
        
        model_files = list(model_dir.glob('ml_model_v4_*.joblib'))
        if not model_files:
            logger.info("No trained models found")
            self._create_lightweight_mock_model()
            return
        
        latest_model = max(model_files, key=lambda x: x.stat().st_mtime)
        
        try:
            model_data = joblib.load(latest_model)
            self.calibrated_pipeline = model_data.get('calibrated_pipeline')
            self.feature_columns = model_data.get('feature_columns', [])
            self.categorical_columns = model_data.get('categorical_columns', [])
            self.numerical_columns = model_data.get('numerical_columns', [])
            self.model_info = model_data.get('model_info', {})
            self.ev_thresholds = model_data.get('ev_thresholds', {})
            
            logger.info(f"üì• Loaded model from {latest_model}")
            logger.info(f"üìä Model info: {self.model_info.get('model_type', 'unknown')}")
        
        except Exception as e:
            logger.error(f"Error loading model: {e}")
            self.calibrated_pipeline = None
            self._create_lightweight_mock_model()


# Training function
def train_leakage_safe_model():
    """Train a new leakage-safe model."""
    system = MLSystemV4()
    success = system.train_model()
    
    if success:
        return {
            'success': True,
            'message': 'Leakage-safe model trained successfully',
            'model_info': system.model_info
        }
    else:
        return {
            'success': False,
            'message': 'Model training failed'
        }


# Backward compatibility alias
def train_new_model(model_type="leakage_safe"):
    """Train a new ML model - backward compatibility wrapper.
    
    Args:
        model_type: Ignored for v4, always uses leakage-safe training
    """
    return train_leakage_safe_model()


if __name__ == "__main__":
    # Test the system
    logger.info("üß™ Testing ML System V4...")
    
    system = MLSystemV4()
    
    # Train model
    success = system.train_model()
    
    if success:
        logger.info("‚úÖ ML System V4 training completed successfully")
    else:
        logger.error("‚ùå ML System V4 training failed")
