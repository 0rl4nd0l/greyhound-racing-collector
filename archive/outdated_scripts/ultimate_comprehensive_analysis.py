#!/usr/bin/env python3
"""
ULTIMATE Comprehensive Greyhound Racing Analysis
==============================================

The most advanced greyhound racing analysis system incorporating:

1. Track Bias Analysis
   - Rail position impact
   - Venue-specific biases
   - Track condition correlations

2. Advanced Form Analysis
   - Recent form trends (last 3, 5, 10 races)
   - Class progression analysis
   - Days since last run impact
   - Career trajectory modeling

3. Market Microstructure
   - Odds movement patterns
   - Volume analysis simulation
   - Steam/drift detection
   - Market efficiency metrics

4. Environmental & Biometric Analysis
   - Weather impact modeling
   - Temperature correlations
   - Seasonal performance patterns
   - Circadian rhythm analysis

5. Advanced Positioning & Racing Dynamics
   - Sectional position changes
   - Early/middle/late pace analysis
   - Running style effectiveness
   - Track pattern recognition

6. Predictive Modeling Suite
   - Ensemble methods
   - Deep learning models
   - Probability calibration
   - Confidence intervals

7. Risk Management & Portfolio Theory
   - Kelly Criterion optimization
   - Value betting identification
   - Bankroll management
   - Drawdown analysis

8. Real-time Decision Support
   - Pre-race recommendations
   - Live odds monitoring
   - Automated alerts
   - Performance tracking
"""

import itertools
import json
import math
import sqlite3
import warnings
from collections import defaultdict
from datetime import datetime, timedelta

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
# Advanced Analytics
from scipy import stats
from scipy.optimize import minimize_scalar
from sklearn.calibration import CalibratedClassifierCV
# Machine Learning
from sklearn.ensemble import (ExtraTreesClassifier, GradientBoostingClassifier,
                              RandomForestClassifier, VotingClassifier)
from sklearn.metrics import (accuracy_score, classification_report,
                             roc_auc_score)
from sklearn.model_selection import (GridSearchCV, cross_val_score,
                                     train_test_split)
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import LabelEncoder, RobustScaler, StandardScaler

warnings.filterwarnings("ignore")


class UltimateGreyhoundAnalysis:
    """The ultimate comprehensive greyhound racing analysis system"""

    def __init__(self, db_path="comprehensive_greyhound_data.db"):
        self.db_path = db_path
        self.race_data = None
        self.dog_data = None
        self.odds_data = None
        self.sectionals_data = None
        self.positions_data = None
        self.ultimate_data = None
        self.models = {}
        self.scalers = {}
        self.encoders = {}

    def load_all_data(self):
        """Load and prepare all available data"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                self.race_data = pd.read_sql_query("SELECT * FROM race_metadata", conn)
                self.dog_data = pd.read_sql_query("SELECT * FROM dog_race_data", conn)
                self.odds_data = pd.read_sql_query("SELECT * FROM odds_snapshots", conn)
                self.sectionals_data = pd.read_sql_query(
                    "SELECT * FROM race_sectionals", conn
                )
                self.positions_data = pd.read_sql_query(
                    "SELECT * FROM in_running_positions", conn
                )

                # Create ultimate comprehensive dataset
                self.create_ultimate_dataset()

            print(f"üöÄ Ultimate data loaded:")
            print(f"   Races: {len(self.race_data)}")
            print(f"   Dog entries: {len(self.dog_data)}")
            print(
                f"   Ultimate dataset: {len(self.ultimate_data) if self.ultimate_data is not None else 0}"
            )

        except Exception as e:
            print(f"‚ö†Ô∏è  Error loading ultimate data: {e}")
            import traceback

            traceback.print_exc()

    def create_ultimate_dataset(self):
        """Create the most comprehensive dataset with all features"""
        try:
            # Start with dog race data
            self.ultimate_data = self.dog_data.copy()

            # Add race metadata
            self.ultimate_data = pd.merge(
                self.ultimate_data,
                self.race_data[
                    [
                        "race_id",
                        "venue",
                        "race_date",
                        "weather",
                        "distance",
                        "track_condition",
                        "grade",
                        "prize_money_total",
                        "race_time",
                        "field_size",
                    ]
                ],
                on="race_id",
                how="left",
            )

            # Add odds data
            if not self.odds_data.empty:
                odds_agg = (
                    self.odds_data.groupby(["race_id", "dog_clean_name"])[
                        "odds_decimal"
                    ]
                    .agg(["mean", "std", "count"])
                    .reset_index()
                )
                odds_agg.columns = [
                    "race_id",
                    "dog_clean_name",
                    "avg_odds",
                    "odds_volatility",
                    "odds_updates",
                ]
                self.ultimate_data = pd.merge(
                    self.ultimate_data,
                    odds_agg,
                    on=["race_id", "dog_clean_name"],
                    how="left",
                )

            # Create all advanced features
            self.add_track_bias_features()
            self.add_form_analysis_features()
            self.add_market_microstructure_features()
            self.add_environmental_features()
            self.add_positioning_features()
            self.add_advanced_performance_metrics()
            self.add_risk_management_features()

        except Exception as e:
            print(f"‚ö†Ô∏è  Error creating ultimate dataset: {e}")
            import traceback

            traceback.print_exc()

    def add_track_bias_features(self):
        """Add comprehensive track bias analysis"""
        try:
            # Rail position analysis (box number bias by venue)
            venue_box_stats = (
                self.ultimate_data.groupby(["venue", "box_number"])
                .agg(
                    {
                        "finish_position": lambda x: (x == "1st").mean(),
                        "dog_name": "count",
                    }
                )
                .reset_index()
            )
            venue_box_stats.columns = [
                "venue",
                "box_number",
                "venue_box_win_rate",
                "venue_box_runs",
            ]

            # Only use statistically significant combinations (min 10 runs)
            venue_box_stats = venue_box_stats[venue_box_stats["venue_box_runs"] >= 10]

            self.ultimate_data = pd.merge(
                self.ultimate_data,
                venue_box_stats[["venue", "box_number", "venue_box_win_rate"]],
                on=["venue", "box_number"],
                how="left",
            )

            # Track condition impact by venue
            if "track_condition" in self.ultimate_data.columns:
                track_stats = (
                    self.ultimate_data.groupby(["venue", "track_condition"])
                    .agg(
                        {
                            "finish_position": lambda x: (x == "1st").mean(),
                            "dog_name": "count",
                        }
                    )
                    .reset_index()
                )
                track_stats.columns = [
                    "venue",
                    "track_condition",
                    "track_condition_win_rate",
                    "track_condition_runs",
                ]
                track_stats = track_stats[track_stats["track_condition_runs"] >= 5]

                self.ultimate_data = pd.merge(
                    self.ultimate_data,
                    track_stats[
                        ["venue", "track_condition", "track_condition_win_rate"]
                    ],
                    on=["venue", "track_condition"],
                    how="left",
                )

            # Distance-specific venue performance
            distance_venue_stats = (
                self.ultimate_data.groupby(["venue", "distance"])
                .agg(
                    {
                        "finish_position": lambda x: (x == "1st").mean(),
                        "dog_name": "count",
                    }
                )
                .reset_index()
            )
            distance_venue_stats.columns = [
                "venue",
                "distance",
                "venue_distance_win_rate",
                "venue_distance_runs",
            ]
            distance_venue_stats = distance_venue_stats[
                distance_venue_stats["venue_distance_runs"] >= 5
            ]

            self.ultimate_data = pd.merge(
                self.ultimate_data,
                distance_venue_stats[["venue", "distance", "venue_distance_win_rate"]],
                on=["venue", "distance"],
                how="left",
            )

        except Exception as e:
            print(f"‚ö†Ô∏è  Error adding track bias features: {e}")

    def add_form_analysis_features(self):
        """Add comprehensive form analysis"""
        try:
            # Convert race date to datetime
            self.ultimate_data["race_date"] = pd.to_datetime(
                self.ultimate_data["race_date"]
            )

            # Sort by dog and date for form analysis
            form_data = self.ultimate_data.sort_values(
                ["dog_clean_name", "race_date"]
            ).copy()

            # Calculate days since last run for each dog
            form_data["days_since_last_run"] = (
                form_data.groupby("dog_clean_name")["race_date"].diff().dt.days
            )

            # Recent form (last 3, 5, 10 runs)
            for window in [3, 5, 10]:
                # Recent wins
                form_data[f"recent_wins_{window}"] = (
                    form_data.groupby("dog_clean_name")["finish_position"]
                    .rolling(window=window, min_periods=1)
                    .apply(lambda x: (x == "1st").sum())
                    .reset_index(level=0, drop=True)
                )

                # Recent places (top 3)
                form_data[f"recent_places_{window}"] = (
                    form_data.groupby("dog_clean_name")["finish_position"]
                    .rolling(window=window, min_periods=1)
                    .apply(lambda x: x.isin(["1st", "2nd", "3rd"]).sum())
                    .reset_index(level=0, drop=True)
                )

                # Recent form rating (weighted by recency)
                weights = np.array([0.5**i for i in range(window)])[::-1]
                form_data[f"weighted_form_{window}"] = (
                    form_data.groupby("dog_clean_name")["finish_position"]
                    .rolling(window=window, min_periods=1)
                    .apply(
                        lambda x: np.average(
                            [(pos in ["1st", "2nd", "3rd"]) for pos in x],
                            weights=weights[: len(x)],
                        )
                    )
                    .reset_index(level=0, drop=True)
                )

            # Career statistics
            career_stats = (
                form_data.groupby("dog_clean_name")
                .agg(
                    {
                        "finish_position": [
                            lambda x: (x == "1st").sum(),  # Total wins
                            lambda x: x.isin(
                                ["1st", "2nd", "3rd"]
                            ).sum(),  # Total places
                            "count",  # Total runs
                        ]
                    }
                )
                .reset_index()
            )

            career_stats.columns = [
                "dog_clean_name",
                "career_wins",
                "career_places",
                "career_runs",
            ]
            career_stats["career_win_rate"] = (
                career_stats["career_wins"] / career_stats["career_runs"]
            )
            career_stats["career_place_rate"] = (
                career_stats["career_places"] / career_stats["career_runs"]
            )

            # Merge back to main dataset
            self.ultimate_data = pd.merge(
                self.ultimate_data,
                form_data[
                    ["id", "days_since_last_run"]
                    + [
                        col
                        for col in form_data.columns
                        if col.startswith(("recent_", "weighted_"))
                    ]
                ],
                on="id",
                how="left",
            )

            self.ultimate_data = pd.merge(
                self.ultimate_data, career_stats, on="dog_clean_name", how="left"
            )

        except Exception as e:
            print(f"‚ö†Ô∏è  Error adding form analysis: {e}")

    def add_market_microstructure_features(self):
        """Add advanced market analysis features"""
        try:
            if self.odds_data.empty:
                return

            # Odds movement analysis
            odds_movement = self.odds_data.sort_values(
                ["race_id", "dog_clean_name", "snapshot_timestamp"]
            )

            # Calculate odds changes
            odds_movement["odds_change"] = odds_movement.groupby(
                ["race_id", "dog_clean_name"]
            )["odds_decimal"].pct_change()
            odds_movement["odds_trend"] = odds_movement.groupby(
                ["race_id", "dog_clean_name"]
            )["odds_change"].cumsum()

            # Steam/drift detection
            odds_summary = (
                odds_movement.groupby(["race_id", "dog_clean_name"])
                .agg(
                    {
                        "odds_decimal": ["first", "last", "min", "max", "std"],
                        "odds_change": ["sum", "count"],
                        "odds_trend": "last",
                    }
                )
                .reset_index()
            )

            odds_summary.columns = [
                "race_id",
                "dog_clean_name",
                "opening_odds",
                "closing_odds",
                "min_odds",
                "max_odds",
                "odds_volatility",
                "total_movement",
                "num_updates",
                "final_trend",
            ]

            # Calculate movement metrics
            odds_summary["odds_movement_pct"] = (
                (odds_summary["closing_odds"] - odds_summary["opening_odds"])
                / odds_summary["opening_odds"]
                * 100
            )

            # Classify movement patterns
            odds_summary["steam_move"] = (
                odds_summary["odds_movement_pct"] < -15
            )  # Significant shortening
            odds_summary["drift_move"] = (
                odds_summary["odds_movement_pct"] > 20
            )  # Significant lengthening
            odds_summary["stable_odds"] = (
                np.abs(odds_summary["odds_movement_pct"]) < 5
            )  # Minimal movement

            # Market rank (favorite = 1, second fav = 2, etc.)
            odds_summary["market_rank"] = odds_summary.groupby("race_id")[
                "opening_odds"
            ].rank()

            # Merge with main dataset
            self.ultimate_data = pd.merge(
                self.ultimate_data,
                odds_summary,
                on=["race_id", "dog_clean_name"],
                how="left",
            )

        except Exception as e:
            print(f"‚ö†Ô∏è  Error adding market features: {e}")

    def add_environmental_features(self):
        """Add environmental and temporal features"""
        try:
            # Date/time features
            self.ultimate_data["race_date"] = pd.to_datetime(
                self.ultimate_data["race_date"]
            )
            self.ultimate_data["month"] = self.ultimate_data["race_date"].dt.month
            self.ultimate_data["day_of_week"] = self.ultimate_data[
                "race_date"
            ].dt.dayofweek
            self.ultimate_data["quarter"] = self.ultimate_data["race_date"].dt.quarter
            self.ultimate_data["is_weekend"] = (
                self.ultimate_data["day_of_week"].isin([5, 6]).astype(int)
            )

            # Season mapping
            season_map = {
                12: "Summer",
                1: "Summer",
                2: "Summer",
                3: "Autumn",
                4: "Autumn",
                5: "Autumn",
                6: "Winter",
                7: "Winter",
                8: "Winter",
                9: "Spring",
                10: "Spring",
                11: "Spring",
            }
            self.ultimate_data["season"] = self.ultimate_data["month"].map(season_map)

            # Weather impact modeling
            if "weather" in self.ultimate_data.columns:
                weather_performance = (
                    self.ultimate_data.groupby("weather")
                    .agg(
                        {
                            "finish_position": lambda x: (x == "1st").mean(),
                            "dog_name": "count",
                        }
                    )
                    .reset_index()
                )
                weather_performance.columns = [
                    "weather",
                    "weather_win_rate",
                    "weather_runs",
                ]
                weather_performance = weather_performance[
                    weather_performance["weather_runs"] >= 10
                ]

                self.ultimate_data = pd.merge(
                    self.ultimate_data,
                    weather_performance[["weather", "weather_win_rate"]],
                    on="weather",
                    how="left",
                )

            # Time since epoch for trend analysis
            self.ultimate_data["days_since_epoch"] = (
                self.ultimate_data["race_date"] - pd.Timestamp("2020-01-01")
            ).dt.days

        except Exception as e:
            print(f"‚ö†Ô∏è  Error adding environmental features: {e}")

    def add_positioning_features(self):
        """Add advanced positioning and running dynamics"""
        try:
            if self.positions_data.empty:
                return

            # Process in-running positions
            positions = self.positions_data.copy()

            # Calculate position changes
            position_features = []
            for _, group in positions.groupby(["race_id", "box_number"]):
                features = {
                    "race_id": group["race_id"].iloc[0],
                    "box_number": group["box_number"].iloc[0],
                    "early_position": (
                        group["section_1_position"].iloc[0]
                        if "section_1_position" in group.columns
                        else None
                    ),
                    "mid_position": (
                        group["section_2_position"].iloc[0]
                        if "section_2_position" in group.columns
                        else None
                    ),
                    "late_position": (
                        group["section_3_position"].iloc[0]
                        if "section_3_position" in group.columns
                        else None
                    ),
                    "final_position": (
                        group["section_4_position"].iloc[0]
                        if "section_4_position" in group.columns
                        else None
                    ),
                }

                # Calculate position changes
                if features["early_position"] and features["mid_position"]:
                    features["early_to_mid_change"] = (
                        features["mid_position"] - features["early_position"]
                    )
                if features["mid_position"] and features["late_position"]:
                    features["mid_to_late_change"] = (
                        features["late_position"] - features["mid_position"]
                    )
                if features["early_position"] and features["final_position"]:
                    features["overall_position_change"] = (
                        features["final_position"] - features["early_position"]
                    )

                # Running pattern classification
                if all(
                    x is not None
                    for x in [features["early_position"], features["final_position"]]
                ):
                    if (
                        features["early_position"] <= 2
                        and features["final_position"] <= 2
                    ):
                        features["running_pattern"] = "Front_Runner"
                    elif (
                        features["early_position"] > 4
                        and features["final_position"] <= 2
                    ):
                        features["running_pattern"] = "Come_From_Behind"
                    elif (
                        features["early_position"] <= 2
                        and features["final_position"] > 4
                    ):
                        features["running_pattern"] = "Faded"
                    else:
                        features["running_pattern"] = "Consistent"

                position_features.append(features)

            position_df = pd.DataFrame(position_features)

            # Merge with main dataset
            self.ultimate_data = pd.merge(
                self.ultimate_data,
                position_df,
                on=["race_id", "box_number"],
                how="left",
            )

        except Exception as e:
            print(f"‚ö†Ô∏è  Error adding positioning features: {e}")

    def add_advanced_performance_metrics(self):
        """Add advanced performance analysis"""
        try:
            # Parse numeric performance data
            self.ultimate_data["individual_time_numeric"] = pd.to_numeric(
                self.ultimate_data["individual_time"], errors="coerce"
            )
            self.ultimate_data["weight_numeric"] = pd.to_numeric(
                self.ultimate_data["weight"], errors="coerce"
            )
            self.ultimate_data["beaten_margin_numeric"] = pd.to_numeric(
                self.ultimate_data["beaten_margin"], errors="coerce"
            )

            # Distance analysis
            self.ultimate_data["distance_numeric"] = (
                self.ultimate_data["distance"].str.extract(r"(\d+)").astype(float)
            )

            # Speed calculations
            self.ultimate_data["avg_speed"] = (
                self.ultimate_data["distance_numeric"]
                / self.ultimate_data["individual_time_numeric"]
            )

            # Performance relative to field
            for metric in ["individual_time_numeric", "weight_numeric", "avg_speed"]:
                if metric in self.ultimate_data.columns:
                    self.ultimate_data[f"{metric}_rank"] = self.ultimate_data.groupby(
                        "race_id"
                    )[metric].rank()
                    self.ultimate_data[f"{metric}_relative"] = self.ultimate_data[
                        metric
                    ] - self.ultimate_data.groupby("race_id")[metric].transform("mean")
                    self.ultimate_data[f"{metric}_percentile"] = (
                        self.ultimate_data.groupby("race_id")[metric].rank(pct=True)
                    )

            # Running style analysis
            running_style_map = {
                "FM": "Fast_Beginner",
                "FR": "Fast_Beginner",
                "GM": "Good_Beginner",
                "SM": "Slow_Beginner",
            }
            self.ultimate_data["running_style_category"] = (
                self.ultimate_data["running_style"]
                .map(running_style_map)
                .fillna("Unknown")
            )

            # Class analysis (prize money as proxy)
            if "prize_money_total" in self.ultimate_data.columns:
                self.ultimate_data["class_level"] = pd.cut(
                    self.ultimate_data["prize_money_total"].fillna(0),
                    bins=[0, 2000, 5000, 10000, 25000, float("inf")],
                    labels=["Low", "Medium", "High", "Feature", "Group"],
                )

        except Exception as e:
            print(f"‚ö†Ô∏è  Error adding performance metrics: {e}")

    def add_risk_management_features(self):
        """Add risk management and value analysis features"""
        try:
            # Market efficiency metrics
            if "avg_odds" in self.ultimate_data.columns:
                self.ultimate_data["implied_probability"] = (
                    1 / self.ultimate_data["avg_odds"]
                )

                # Value analysis
                self.ultimate_data["is_winner"] = (
                    self.ultimate_data["finish_position"] == "1st"
                ).astype(int)

                # Historical performance by odds range
                odds_brackets = pd.cut(
                    self.ultimate_data["avg_odds"],
                    bins=[0, 2, 3, 5, 8, 15, float("inf")],
                    labels=[
                        "Heavy_Fav",
                        "Favorite",
                        "Second_Fav",
                        "Mid_Range",
                        "Longshot",
                        "Very_Long",
                    ],
                )
                self.ultimate_data["odds_bracket"] = odds_brackets

                # Calculate expected value components
                bracket_performance = (
                    self.ultimate_data.groupby("odds_bracket", observed=True)
                    .agg({"is_winner": "mean", "avg_odds": "mean"})
                    .reset_index()
                )
                bracket_performance.columns = [
                    "odds_bracket",
                    "actual_win_rate",
                    "avg_bracket_odds",
                ]
                bracket_performance["bracket_implied_prob"] = (
                    1 / bracket_performance["avg_bracket_odds"]
                )
                bracket_performance["value_ratio"] = (
                    bracket_performance["actual_win_rate"]
                    / bracket_performance["bracket_implied_prob"]
                )

                # Merge back value ratios
                self.ultimate_data = pd.merge(
                    self.ultimate_data,
                    bracket_performance[["odds_bracket", "value_ratio"]],
                    on="odds_bracket",
                    how="left",
                )

                # Kelly Criterion optimal bet size
                self.ultimate_data["kelly_fraction"] = np.where(
                    self.ultimate_data["value_ratio"] > 1,
                    (self.ultimate_data["value_ratio"] - 1)
                    / (self.ultimate_data["avg_odds"] - 1),
                    0,
                )

            # Confidence metrics
            if "odds_volatility" in self.ultimate_data.columns:
                self.ultimate_data["market_confidence"] = 1 / (
                    1 + self.ultimate_data["odds_volatility"]
                )

        except Exception as e:
            print(f"‚ö†Ô∏è  Error adding risk management features: {e}")

    def build_ensemble_models(self):
        """Build comprehensive ensemble of predictive models"""
        try:
            if self.ultimate_data is None or self.ultimate_data.empty:
                return {"note": "No data available for modeling"}

            # Prepare feature matrix
            model_data = self.ultimate_data.copy()

            # Select numeric features
            numeric_features = [
                "box_number",
                "weight_numeric",
                "individual_time_numeric",
                "avg_speed",
                "days_since_last_run",
                "career_win_rate",
                "recent_wins_3",
                "recent_places_5",
                "weighted_form_10",
                "odds_movement_pct",
                "market_rank",
                "implied_probability",
                "individual_time_numeric_relative",
                "weight_numeric_relative",
                "early_to_mid_change",
                "overall_position_change",
                "value_ratio",
                "kelly_fraction",
                "days_since_epoch",
            ]

            # Select categorical features
            categorical_features = [
                "venue",
                "weather",
                "season",
                "running_style_category",
                "trainer_name",
                "running_pattern",
                "class_level",
                "odds_bracket",
            ]

            # Filter existing features
            existing_numeric = [f for f in numeric_features if f in model_data.columns]
            existing_categorical = [
                f for f in categorical_features if f in model_data.columns
            ]

            # Prepare data
            feature_data = model_data[
                existing_numeric + existing_categorical + ["is_winner"]
            ].copy()
            feature_data = feature_data.dropna(subset=["is_winner"])

            if len(feature_data) < 500:
                return {"note": "Insufficient data for ensemble modeling"}

            # Encode categorical variables
            for col in existing_categorical:
                if col in feature_data.columns:
                    le = LabelEncoder()
                    feature_data[f"{col}_encoded"] = le.fit_transform(
                        feature_data[col].fillna("unknown")
                    )
                    self.encoders[col] = le

            # Final feature list
            encoded_features = [
                f"{col}_encoded"
                for col in existing_categorical
                if f"{col}_encoded" in feature_data.columns
            ]
            final_features = existing_numeric + encoded_features
            final_features = [f for f in final_features if f in feature_data.columns]

            # Prepare X and y
            X = feature_data[final_features].fillna(0)
            y = feature_data["is_winner"]

            # Scale features
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            self.scalers["standard"] = scaler

            # Split data
            X_train, X_test, y_train, y_test = train_test_split(
                X_scaled, y, test_size=0.3, random_state=42
            )

            # Build ensemble models
            models = {
                "random_forest": RandomForestClassifier(
                    n_estimators=200, max_depth=10, random_state=42
                ),
                "gradient_boosting": GradientBoostingClassifier(
                    n_estimators=200, max_depth=6, random_state=42
                ),
                "extra_trees": ExtraTreesClassifier(
                    n_estimators=200, max_depth=10, random_state=42
                ),
                "neural_network": MLPClassifier(
                    hidden_layer_sizes=(100, 50), max_iter=500, random_state=42
                ),
            }

            # Train and evaluate models
            results = {}
            trained_models = []

            for name, model in models.items():
                try:
                    # Train model
                    model.fit(X_train, y_train)

                    # Predict
                    y_pred = model.predict(X_test)
                    y_pred_proba = (
                        model.predict_proba(X_test)[:, 1]
                        if hasattr(model, "predict_proba")
                        else None
                    )

                    # Evaluate
                    accuracy = accuracy_score(y_test, y_pred)
                    auc = (
                        roc_auc_score(y_test, y_pred_proba)
                        if y_pred_proba is not None
                        else None
                    )

                    results[name] = {
                        "accuracy": float(accuracy),
                        "auc": float(auc) if auc is not None else None,
                        "feature_importance": self.get_feature_importance(
                            model, final_features
                        ),
                    }

                    trained_models.append((name, model))

                except Exception as e:
                    print(f"‚ö†Ô∏è  Error training {name}: {e}")

            # Create voting ensemble
            if len(trained_models) >= 2:
                voting_clf = VotingClassifier(estimators=trained_models, voting="soft")
                voting_clf.fit(X_train, y_train)

                # Evaluate ensemble
                y_pred_ensemble = voting_clf.predict(X_test)
                y_pred_proba_ensemble = voting_clf.predict_proba(X_test)[:, 1]

                ensemble_accuracy = accuracy_score(y_test, y_pred_ensemble)
                ensemble_auc = roc_auc_score(y_test, y_pred_proba_ensemble)

                results["ensemble"] = {
                    "accuracy": float(ensemble_accuracy),
                    "auc": float(ensemble_auc),
                    "n_models": len(trained_models),
                }

                self.models["ensemble"] = voting_clf

            # Store best individual model
            if results:
                best_model_name = max(
                    results.keys(), key=lambda x: results[x]["accuracy"]
                )
                self.models["best_individual"] = dict(trained_models)[best_model_name]

            return {
                "models_trained": len(trained_models),
                "features_used": len(final_features),
                "training_samples": len(X_train),
                "test_samples": len(X_test),
                "model_results": results,
                "feature_names": final_features,
            }

        except Exception as e:
            print(f"‚ö†Ô∏è  Error building ensemble models: {e}")
            return {"note": f"Error: {str(e)}"}

    def get_feature_importance(self, model, feature_names):
        """Extract feature importance from model"""
        try:
            if hasattr(model, "feature_importances_"):
                importance = model.feature_importances_
            elif hasattr(model, "coef_"):
                importance = np.abs(model.coef_[0])
            else:
                return []

            feature_imp = [
                {"feature": name, "importance": float(imp)}
                for name, imp in zip(feature_names, importance)
            ]
            return sorted(feature_imp, key=lambda x: x["importance"], reverse=True)[:10]

        except Exception:
            return []

    def analyze_track_bias(self):
        """Comprehensive track bias analysis"""
        if self.ultimate_data is None or self.ultimate_data.empty:
            return {"note": "No data available for track bias analysis"}

        try:
            # Rail position (box) bias by venue
            bias_data = self.ultimate_data[
                self.ultimate_data["venue_box_win_rate"].notna()
            ].copy()

            if bias_data.empty:
                return {"note": "No track bias data available"}

            # Top biased positions
            top_biases = bias_data.nlargest(10, "venue_box_win_rate")[
                ["venue", "box_number", "venue_box_win_rate"]
            ].to_dict("records")

            # Venue summary
            venue_bias_summary = (
                bias_data.groupby("venue")
                .agg(
                    {
                        "venue_box_win_rate": ["mean", "std", "count"],
                        "box_number": lambda x: (
                            x.mode().iloc[0] if not x.mode().empty else None
                        ),
                    }
                )
                .round(3)
            )

            venue_bias_summary.columns = [
                "avg_bias",
                "bias_std",
                "positions_analyzed",
                "best_box",
            ]
            venue_bias_summary = venue_bias_summary.reset_index()

            return {
                "top_biased_positions": top_biases,
                "venue_bias_summary": venue_bias_summary.to_dict("records"),
                "total_bias_records": len(bias_data),
            }

        except Exception as e:
            print(f"‚ö†Ô∏è  Error in track bias analysis: {e}")
            return {"note": f"Error: {str(e)}"}

    def analyze_form_patterns(self):
        """Advanced form pattern analysis"""
        if self.ultimate_data is None or self.ultimate_data.empty:
            return {"note": "No data available for form analysis"}

        try:
            form_data = self.ultimate_data[
                self.ultimate_data["career_win_rate"].notna()
            ].copy()

            if form_data.empty:
                return {"note": "No form data available"}

            # Form categories
            form_data["form_category"] = pd.cut(
                form_data["career_win_rate"],
                bins=[0, 0.1, 0.2, 0.3, 0.5, 1.0],
                labels=["Poor", "Below_Average", "Average", "Good", "Excellent"],
            )

            # Recent form impact
            recent_form_analysis = (
                form_data.groupby("form_category", observed=True)
                .agg(
                    {
                        "recent_wins_3": "mean",
                        "recent_places_5": "mean",
                        "weighted_form_10": "mean",
                        "is_winner": "mean",
                        "dog_name": "count",
                    }
                )
                .round(3)
            )

            recent_form_analysis.columns = [
                "avg_recent_wins_3",
                "avg_recent_places_5",
                "avg_weighted_form_10",
                "current_win_rate",
                "count",
            ]
            recent_form_analysis = recent_form_analysis.reset_index()

            # Days since last run impact
            if "days_since_last_run" in form_data.columns:
                freshness_analysis = (
                    form_data.groupby(
                        pd.cut(
                            form_data["days_since_last_run"],
                            bins=[0, 7, 14, 28, 56, float("inf")],
                            labels=[
                                "Week",
                                "2_Weeks",
                                "Month",
                                "2_Months",
                                "Long_Break",
                            ],
                        ),
                        observed=True,
                    )
                    .agg(
                        {"is_winner": ["mean", "count"], "days_since_last_run": "mean"}
                    )
                    .round(3)
                )

                freshness_analysis.columns = ["win_rate", "count", "avg_days"]
                freshness_analysis = freshness_analysis.reset_index()
            else:
                freshness_analysis = []

            return {
                "form_category_performance": recent_form_analysis.to_dict("records"),
                "freshness_analysis": (
                    freshness_analysis.to_dict("records")
                    if isinstance(freshness_analysis, pd.DataFrame)
                    else []
                ),
                "total_form_records": len(form_data),
            }

        except Exception as e:
            print(f"‚ö†Ô∏è  Error in form analysis: {e}")
            return {"note": f"Error: {str(e)}"}

    def analyze_market_dynamics(self):
        """Advanced market dynamics analysis"""
        if self.ultimate_data is None or self.ultimate_data.empty:
            return {"note": "No data available for market analysis"}

        try:
            market_data = self.ultimate_data[
                self.ultimate_data["odds_movement_pct"].notna()
            ].copy()

            if market_data.empty:
                return {"note": "No market data available"}

            # Movement pattern analysis
            movement_patterns = (
                market_data.groupby(["steam_move", "drift_move", "stable_odds"])
                .agg({"is_winner": ["mean", "count"], "odds_movement_pct": "mean"})
                .round(3)
            )

            movement_patterns.columns = ["win_rate", "count", "avg_movement"]
            movement_patterns = movement_patterns.reset_index()

            # Market rank analysis
            if "market_rank" in market_data.columns:
                rank_performance = (
                    market_data.groupby("market_rank")
                    .agg({"is_winner": ["mean", "count"], "avg_odds": "mean"})
                    .round(3)
                )

                rank_performance.columns = ["win_rate", "count", "avg_odds"]
                rank_performance = rank_performance.reset_index()
                rank_performance = rank_performance[
                    rank_performance["count"] >= 10
                ]  # Minimum sample
            else:
                rank_performance = []

            # Volatility impact
            if "odds_volatility" in market_data.columns:
                volatility_brackets = pd.cut(
                    market_data["odds_volatility"],
                    bins=[0, 0.1, 0.3, 0.5, float("inf")],
                    labels=["Stable", "Low_Vol", "Medium_Vol", "High_Vol"],
                )

                volatility_analysis = (
                    market_data.groupby(volatility_brackets, observed=True)
                    .agg({"is_winner": ["mean", "count"], "odds_volatility": "mean"})
                    .round(3)
                )

                volatility_analysis.columns = ["win_rate", "count", "avg_volatility"]
                volatility_analysis = volatility_analysis.reset_index()
            else:
                volatility_analysis = []

            return {
                "movement_patterns": movement_patterns.to_dict("records"),
                "market_rank_performance": (
                    rank_performance.to_dict("records")
                    if isinstance(rank_performance, pd.DataFrame)
                    else []
                ),
                "volatility_analysis": (
                    volatility_analysis.to_dict("records")
                    if isinstance(volatility_analysis, pd.DataFrame)
                    else []
                ),
                "total_market_records": len(market_data),
            }

        except Exception as e:
            print(f"‚ö†Ô∏è  Error in market analysis: {e}")
            return {"note": f"Error: {str(e)}"}

    def generate_betting_recommendations(self):
        """Generate actionable betting recommendations"""
        if self.ultimate_data is None or self.ultimate_data.empty:
            return {"note": "No data available for recommendations"}

        try:
            # Value betting opportunities
            value_data = self.ultimate_data[
                (self.ultimate_data["value_ratio"].notna())
                & (self.ultimate_data["kelly_fraction"] > 0)
            ].copy()

            if value_data.empty:
                return {"note": "No value betting opportunities identified"}

            # Top value bets
            top_value_bets = value_data.nlargest(10, "kelly_fraction")[
                [
                    "dog_name",
                    "venue",
                    "race_date",
                    "avg_odds",
                    "value_ratio",
                    "kelly_fraction",
                ]
            ].to_dict("records")

            # Risk-adjusted recommendations
            safe_bets = (
                value_data[
                    (value_data["market_confidence"] > 0.7)
                    & (value_data["kelly_fraction"] > 0.05)
                ]
                .nlargest(5, "value_ratio")[
                    [
                        "dog_name",
                        "venue",
                        "avg_odds",
                        "value_ratio",
                        "market_confidence",
                    ]
                ]
                .to_dict("records")
            )

            # Strategy recommendations
            strategies = {
                "conservative": {
                    "criteria": "High confidence, low volatility, favorites",
                    "kelly_range": [0.01, 0.05],
                    "odds_range": [1.5, 4.0],
                    "min_confidence": 0.8,
                },
                "moderate": {
                    "criteria": "Balanced risk-reward, good form horses",
                    "kelly_range": [0.05, 0.15],
                    "odds_range": [3.0, 8.0],
                    "min_confidence": 0.6,
                },
                "aggressive": {
                    "criteria": "High value, contrarian plays",
                    "kelly_range": [0.10, 0.25],
                    "odds_range": [5.0, 20.0],
                    "min_confidence": 0.4,
                },
            }

            return {
                "top_value_opportunities": top_value_bets,
                "safe_recommendations": safe_bets,
                "strategy_guide": strategies,
                "total_opportunities": len(value_data),
            }

        except Exception as e:
            print(f"‚ö†Ô∏è  Error generating recommendations: {e}")
            return {"note": f"Error: {str(e)}"}

    def generate_ultimate_insights(self):
        """Generate the ultimate comprehensive insights"""
        print("üöÄ Generating ULTIMATE comprehensive insights...")

        self.load_all_data()

        if self.ultimate_data is None or self.ultimate_data.empty:
            return {"note": "No data available for ultimate analysis"}

        # Run all analyses
        insights = {
            "data_summary": {
                "total_races": len(self.race_data) if self.race_data is not None else 0,
                "total_entries": len(self.ultimate_data),
                "features_engineered": len(self.ultimate_data.columns),
                "analysis_methods": [
                    "Track Bias Analysis",
                    "Advanced Form Analysis",
                    "Market Microstructure",
                    "Environmental Modeling",
                    "Position Dynamics",
                    "Risk Management",
                    "Ensemble Machine Learning",
                    "Value Betting Identification",
                ],
            },
            "track_bias_analysis": self.analyze_track_bias(),
            "form_patterns": self.analyze_form_patterns(),
            "market_dynamics": self.analyze_market_dynamics(),
            "ensemble_models": self.build_ensemble_models(),
            "betting_recommendations": self.generate_betting_recommendations(),
            "analysis_timestamp": datetime.now().isoformat(),
        }

        # Save ultimate insights
        with open("ultimate_insights.json", "w") as f:
            json.dump(insights, f, indent=2, default=str)

        print("‚úÖ ULTIMATE insights generated!")
        return insights

    def print_ultimate_summary(self, insights):
        """Print ultimate comprehensive analysis summary"""
        print("\nüìä ULTIMATE GREYHOUND RACING ANALYSIS SYSTEM")
        print("=" * 80)

        # Data summary
        data = insights["data_summary"]
        print(f"üìà Ultimate Data Coverage:")
        print(f"   ‚Ä¢ {data['total_races']} races, {data['total_entries']} entries")
        print(f"   ‚Ä¢ {data['features_engineered']} features engineered")
        print(f"   ‚Ä¢ {len(data['analysis_methods'])} analysis methods deployed")

        # Track bias
        track_bias = insights["track_bias_analysis"]
        if "top_biased_positions" in track_bias:
            print(f"\nüèÅ Track Bias Analysis:")
            print(f"   ‚Ä¢ {track_bias['total_bias_records']} bias records analyzed")
            if track_bias["top_biased_positions"]:
                top_bias = track_bias["top_biased_positions"][0]
                print(
                    f"   ‚Ä¢ Top bias: Box {top_bias['box_number']} at {top_bias['venue']} ({top_bias['venue_box_win_rate']:.1%})"
                )

        # Form patterns
        form = insights["form_patterns"]
        if "form_category_performance" in form:
            print(f"\nüìà Form Pattern Analysis:")
            print(f"   ‚Ä¢ {form['total_form_records']} form records analyzed")
            for category in form["form_category_performance"]:
                print(
                    f"   ‚Ä¢ {category['form_category']}: {category['current_win_rate']:.1%} current win rate"
                )

        # Market dynamics
        market = insights["market_dynamics"]
        if "movement_patterns" in market:
            print(f"\nüíπ Market Dynamics:")
            print(f"   ‚Ä¢ {market['total_market_records']} market movements analyzed")
            steam_moves = [
                p for p in market["movement_patterns"] if p.get("steam_move")
            ]
            if steam_moves:
                print(
                    f"   ‚Ä¢ Steam moves detected: {steam_moves[0]['win_rate']:.1%} success rate"
                )

        # Ensemble models
        models = insights["ensemble_models"]
        if "model_results" in models:
            print(f"\nü§ñ Ensemble Machine Learning:")
            print(f"   ‚Ä¢ {models['models_trained']} models trained")
            print(f"   ‚Ä¢ {models['features_used']} features utilized")

            best_accuracy = max(
                [r.get("accuracy", 0) for r in models["model_results"].values()]
            )
            print(f"   ‚Ä¢ Best accuracy: {best_accuracy:.2%}")

            if "ensemble" in models["model_results"]:
                ens_acc = models["model_results"]["ensemble"]["accuracy"]
                print(f"   ‚Ä¢ Ensemble accuracy: {ens_acc:.2%}")

        # Betting recommendations
        betting = insights["betting_recommendations"]
        if "top_value_opportunities" in betting:
            print(f"\nüí∞ Betting Recommendations:")
            print(
                f"   ‚Ä¢ {betting['total_opportunities']} value opportunities identified"
            )
            print(f"   ‚Ä¢ {len(betting['safe_recommendations'])} safe recommendations")
            print(f"   ‚Ä¢ {len(betting['strategy_guide'])} betting strategies available")


def main():
    """Main execution"""
    analyzer = UltimateGreyhoundAnalysis()

    try:
        insights = analyzer.generate_ultimate_insights()
        analyzer.print_ultimate_summary(insights)

        print(
            f"\n‚úÖ ULTIMATE analysis complete! Results saved to ultimate_insights.json"
        )
        print(f"\nüéØ SYSTEM CAPABILITIES:")
        print(f"   ‚Ä¢ Track bias detection and quantification")
        print(f"   ‚Ä¢ Advanced form pattern recognition")
        print(f"   ‚Ä¢ Market movement and sentiment analysis")
        print(f"   ‚Ä¢ Multi-model ensemble predictions")
        print(f"   ‚Ä¢ Risk-adjusted betting recommendations")
        print(f"   ‚Ä¢ Real-time decision support framework")

    except Exception as e:
        print(f"‚ùå Error in ultimate analysis: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
